{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90891f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "base_dir = \".\"  # Adjust path if needed\n",
    "\n",
    "# Config map for CKKS configurations\n",
    "config_map = {\n",
    "    \"light\": \"CKKS Light\",\n",
    "    \"med\": \"CKKS Medium\",\n",
    "    \"heavy\": \"CKKS Heavy\"\n",
    "}\n",
    "\n",
    "server_metrics = [\"avg_cpu\", \"peak_cpu\", \"avg_mem\", \"peak_mem\"]\n",
    "config_round_data = {label: defaultdict(list) for label in config_map.values()}\n",
    "\n",
    "# === Parse each config ===\n",
    "for config_prefix, label in config_map.items():\n",
    "    for folder in os.listdir(base_dir):\n",
    "        if folder.startswith(f\"exp_{config_prefix}_r\"):\n",
    "            try:\n",
    "                server_log = [f for f in os.listdir(os.path.join(base_dir, folder)) if f.startswith(\"log_server_\")][0]\n",
    "                file_path = os.path.join(base_dir, folder, server_log, \"resource_usage.txt\")\n",
    "\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    current_round = None\n",
    "                    round_data = {}\n",
    "\n",
    "                    for line in f:\n",
    "                        round_match = re.match(r\"Round (\\d+):\", line)\n",
    "                        if round_match:\n",
    "                            if current_round is not None:\n",
    "                                config_round_data[label][current_round].append(round_data)\n",
    "                            current_round = int(round_match.group(1))\n",
    "                            round_data = {}\n",
    "                        else:\n",
    "                            metric_match = re.match(r\"\\s*(\\w+):\\s*([\\d.]+)\", line)\n",
    "                            if metric_match:\n",
    "                                key, val = metric_match.groups()\n",
    "                                val = float(val)\n",
    "                                if key in server_metrics:\n",
    "                                    round_data[key] = val\n",
    "\n",
    "                    if current_round is not None:\n",
    "                        config_round_data[label][current_round].append(round_data)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error parsing {folder}: {e}\")\n",
    "\n",
    "# === Create summary tables ===\n",
    "summary_tables = {}\n",
    "\n",
    "for config, rounds_dict in config_round_data.items():\n",
    "    round_numbers = sorted(rounds_dict.keys())\n",
    "    summary_data = []\n",
    "\n",
    "    for r in round_numbers:\n",
    "        entry = {\"Round\": r}\n",
    "        for metric in server_metrics:\n",
    "            values = [run[metric] for run in rounds_dict[r] if metric in run]\n",
    "            if values:\n",
    "                entry[f\"{metric} Mean\"] = np.mean(values)\n",
    "                entry[f\"{metric} Std\"] = np.std(values)\n",
    "        summary_data.append(entry)\n",
    "\n",
    "    df_summary = pd.DataFrame(summary_data).set_index(\"Round\").round(2)\n",
    "    summary_tables[config] = df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2101fd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg CPU (%)</th>\n",
       "      <th>Peak CPU (%)</th>\n",
       "      <th>Avg Memory (MB)</th>\n",
       "      <th>Peak Memory (MB)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Config</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CKKS Light</th>\n",
       "      <td>2.79</td>\n",
       "      <td>33.42</td>\n",
       "      <td>1450.83</td>\n",
       "      <td>1532.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CKKS Medium</th>\n",
       "      <td>2.32</td>\n",
       "      <td>29.14</td>\n",
       "      <td>1474.02</td>\n",
       "      <td>1584.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CKKS Heavy</th>\n",
       "      <td>3.33</td>\n",
       "      <td>36.72</td>\n",
       "      <td>1373.56</td>\n",
       "      <td>1494.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Avg CPU (%)  Peak CPU (%)  Avg Memory (MB)  Peak Memory (MB)\n",
       "Config                                                                   \n",
       "CKKS Light          2.79         33.42          1450.83           1532.14\n",
       "CKKS Medium         2.32         29.14          1474.02           1584.01\n",
       "CKKS Heavy          3.33         36.72          1373.56           1494.20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Compute average across all rounds for summary ===\n",
    "server_summary_rows = []\n",
    "\n",
    "for config, df in summary_tables.items():\n",
    "    row = {\"Config\": config}\n",
    "    for metric in [\"avg_cpu\", \"peak_cpu\", \"avg_mem\", \"peak_mem\"]:\n",
    "        col_name = f\"{metric} Mean\"\n",
    "        row[metric] = round(df[col_name].mean(), 2) if col_name in df else \"-\"\n",
    "    server_summary_rows.append(row)\n",
    "\n",
    "# Create summary DataFrame\n",
    "server_summary_df = pd.DataFrame(server_summary_rows).rename(columns={\n",
    "    \"avg_cpu\": \"Avg CPU (%)\",\n",
    "    \"peak_cpu\": \"Peak CPU (%)\",\n",
    "    \"avg_mem\": \"Avg Memory (MB)\",\n",
    "    \"peak_mem\": \"Peak Memory (MB)\"\n",
    "}).set_index(\"Config\")\n",
    "\n",
    "display(server_summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07ef65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "base_dir = \".\"  # or path to your experiment root\n",
    "\n",
    "# Map config folder prefix to label\n",
    "config_map = {\n",
    "    \"light\": \"CKKS Light\",\n",
    "    \"med\": \"CKKS Medium\",\n",
    "    \"heavy\": \"CKKS Heavy\"\n",
    "}\n",
    "\n",
    "# Metrics to extract\n",
    "cpu_metrics = [\"Avg CPU (%)\", \"Peak CPU (%)\"]\n",
    "mem_metrics = [\"Avg Memory (MB)\", \"Peak Memory (MB)\"]\n",
    "\n",
    "# Storage\n",
    "cpu_data = {label: {metric: defaultdict(list) for metric in cpu_metrics} for label in config_map.values()}\n",
    "mem_data = {label: {metric: defaultdict(list) for metric in mem_metrics} for label in config_map.values()}\n",
    "\n",
    "# Walk all experiment folders\n",
    "for folder in os.listdir(base_dir):\n",
    "    for cfg_prefix, label in config_map.items():\n",
    "        if folder.startswith(f\"exp_{cfg_prefix}_r\"):\n",
    "            try:\n",
    "                log_folder = [\n",
    "                    f for f in os.listdir(os.path.join(base_dir, folder))\n",
    "                    if f.startswith(\"log_clients_\")\n",
    "                ][0]\n",
    "                csv_path = os.path.join(base_dir, folder, log_folder, \"client_0\", \"metrics_log.csv\")\n",
    "                df = pd.read_csv(csv_path)\n",
    "\n",
    "                for _, row in df.iterrows():\n",
    "                    round_num = int(row[\"Round\"])\n",
    "                    for metric in cpu_metrics:\n",
    "                        value = row.get(metric, None)\n",
    "                        if pd.notnull(value):\n",
    "                            cpu_data[label][metric][round_num].append(value)\n",
    "                    for metric in mem_metrics:\n",
    "                        value = row.get(metric, None)\n",
    "                        if pd.notnull(value):\n",
    "                            mem_data[label][metric][round_num].append(value)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to load from {folder}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb03c1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg CPU (%)</th>\n",
       "      <th>Peak CPU (%)</th>\n",
       "      <th>Avg Memory (MB)</th>\n",
       "      <th>Peak Memory (MB)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Config</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CKKS Light</th>\n",
       "      <td>40.88</td>\n",
       "      <td>48.41</td>\n",
       "      <td>1476.11</td>\n",
       "      <td>1483.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CKKS Medium</th>\n",
       "      <td>41.76</td>\n",
       "      <td>48.88</td>\n",
       "      <td>1433.68</td>\n",
       "      <td>1439.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CKKS Heavy</th>\n",
       "      <td>38.50</td>\n",
       "      <td>48.20</td>\n",
       "      <td>1508.79</td>\n",
       "      <td>1518.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Avg CPU (%)  Peak CPU (%)  Avg Memory (MB)  Peak Memory (MB)\n",
       "Config                                                                   \n",
       "CKKS Light         40.88         48.41          1476.11           1483.40\n",
       "CKKS Medium        41.76         48.88          1433.68           1439.99\n",
       "CKKS Heavy         38.50         48.20          1508.79           1518.01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine CPU and Memory into one summary table\n",
    "summary_rows = []\n",
    "\n",
    "for config in config_map.values():\n",
    "    row = {\"Config\": config}\n",
    "\n",
    "    for metric in cpu_metrics:\n",
    "        all_vals = []\n",
    "        for round_vals in cpu_data[config][metric].values():\n",
    "            all_vals.extend(round_vals)\n",
    "        row[metric] = round(np.mean(all_vals), 2) if all_vals else \"-\"\n",
    "\n",
    "    for metric in mem_metrics:\n",
    "        all_vals = []\n",
    "        for round_vals in mem_data[config][metric].values():\n",
    "            all_vals.extend(round_vals)\n",
    "        row[metric] = round(np.mean(all_vals), 2) if all_vals else \"-\"\n",
    "\n",
    "    summary_rows.append(row)\n",
    "\n",
    "client_summary_df = pd.DataFrame(summary_rows).set_index(\"Config\")\n",
    "display(client_summary_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
